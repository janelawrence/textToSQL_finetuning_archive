# -*- coding: utf-8 -*-
"""T5_ASSHOLE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/100YdKxpFb-jO_l0pwGLm6qYxkUxh8r1S
"""

# !pip install SentencePiece

# !pip install datasets
# !pip install torch
# !pip install transformers
# !pip install pandas

from datasets import load_dataset
import torch
import json
import os
from transformers import T5Tokenizer, T5ForConditionalGeneration
from transformers import AutoTokenizer
from transformers import TrainingArguments, Trainer

prefix = "Translate text to SQL: "
max_input_length = 2048
max_target_length = 1000

def load_schema_dict(tables):
    db_id_2_schema = dict()
    for i in range(len(tables)):
      db_info = tables[i]
      num_tables_in_db = len(db_info["table_names_original"])
      schemas = ""
      tables_in_db = db_info["table_names_original"]
      cols_in_db = db_info["column_names_original"]
      for j in range(num_tables_in_db):
        table_name_j = tables_in_db[j]
        table_cols_j = [pair[1]  for pair in cols_in_db if pair[0]==j]
        sub_schemas = f"(table_name: {table_name_j}; table_cols: {','.join(table_cols_j)})"
        schemas += sub_schemas
      schema_i = f"num_tables: {num_tables_in_db} {schemas}"
      
      # Summarize joining relations between tables
      joining_summary = ""
      if len(db_info["foreign_keys"]) > 0:
        joining_summary = " Tables could be joined by following rules: "
        counter = 1
        for foreign_pair in db_info["foreign_keys"]:
          t1_idx, t1_f_key = cols_in_db[foreign_pair[0]]
          t2_idx, t2_f_key = cols_in_db[foreign_pair[1]]
          t1_name = tables_in_db[t1_idx]
          t2_name = tables_in_db[t2_idx]
          if(t1_f_key == t2_f_key):
            summary = f"{t1_name} and {t2_name} could be joined on column {t1_f_key}"
          else:
            summary = f"{t1_name} and {t2_name} could be joined on columns {t1_f_key} of {t1_name} and {t2_f_key} of {t2_name}"
          joining_summary += f"{counter}. {summary}\n"
          counter+=1

      schema_i += joining_summary
      db_id_2_schema[tables[i]['db_id']] = schema_i
    return db_id_2_schema

def format_input(question, db_id, schema):
    input = f"{prefix} text: {question} db_id: {db_id} schema: {schema}"
    return input

def format_output(query):
    output = f"{query}"
    return output

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load Dataset
    print("Loading Spider Dataset")
    dataset = load_dataset('spider')
    print("Spider Dataset Loaded")

    dataset_splited = dataset["train"].train_test_split(test_size=0.08, shuffle=True)
    dataset_train = dataset_splited["train"]
    dataset_test = dataset_splited["test"]
    dataset_validation = dataset["validation"]

    # output test set

    # Load Tokenizer
    print("Loading Tokenizer...")
    model_checkpoint = 'google/flan-t5-large'
    print("Model_name: ", model_checkpoint)
    tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-large')
    tokenizer.add_tokens('<')
    
    print("Tokenizer Loaded...")
    print("max model length ", tokenizer.model_max_length)
    
    spider_table_path = os.path.join(os.getcwd(), "tables.json")
    # spider_table_path = "/content/drive/MyDrive/Colab Notebooks/spider/tables.json"
    table_json = json.load(open(spider_table_path, 'r'))
    id_2_schema_dict = load_schema_dict(table_json)

    def preprocess_examples(examples):

        # encode the question-query pairs
        questions = examples['question'] 
        queries = examples['query']
        db_id = examples['db_id']
  
        inputs = [format_input(questions[i], db_id[i], id_2_schema_dict[db_id[i]]) for i in range(len(questions))]
        outputs = [format_output(query) for query in queries]
        print("Input: ", inputs[0], "\nOutput: ", outputs[0])
        
        model_inputs = tokenizer(inputs, max_length=max_input_length, padding="max_length", truncation=True, return_tensors="pt")

        # encode the summaries
        labels = tokenizer(outputs, max_length=max_target_length, padding="max_length", truncation=True, return_tensors="pt").input_ids

        # important: we need to replace the index of the padding tokens by -100
        # such that they are not taken into account by the CrossEntropyLoss
        labels[labels == tokenizer.pad_token_id] = -100
        model_inputs['labels'] = labels
        return model_inputs
    
    # Preprocess data
    print("Preprocess Train and Validation dataset...")
    dataset_train = dataset_train.map(preprocess_examples, batched=True)
    dataset_validation = dataset_validation.map(preprocess_examples, batched=True)
    print("Finished preprocessing data...")

    # Load T5 model
    print("Loading model...")
    model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large")
    model.to(device)
    model.resize_token_embeddings(len(tokenizer))
    print("Finished loading model...")

    # Training the model...
    output_dir = os.getcwd()
    training_args = TrainingArguments(output_dir= output_dir, 
                                  overwrite_output_dir=True,
                                  evaluation_strategy = "epoch",
                                  num_train_epochs=10,
                                  learning_rate=1e-4,
                                  per_device_train_batch_size=4,
                                  per_device_eval_batch_size=4,
                                  fp16=False,
                                  warmup_steps=500, 
                                  weight_decay=0.01)
    
    trainer = Trainer(
        model,
        args=training_args,
        tokenizer=tokenizer,
        train_dataset=dataset_train,
        eval_dataset= dataset_validation,
    )

    print("Starting training...")
    trainer.train()
    print("Finished training...")
    print("Saving the model...")
    trainer.save_model()
    print("Model_saved...")


    # Load trained Model
    print("Loading trained model...")
    fine_tuned_model = T5ForConditionalGeneration.from_pretrained(output_dir)
    fine_tuned_model.to(device)
    fine_tuned_model_tokenizer = AutoTokenizer.from_pretrained(output_dir)
    print("Model Loaded...")

    def remove_pad_token(query):
        query = query.lstrip('<pad>').strip()
        query = query.rstrip('</s>').strip()
        return query

    def infer(input, model, tokenizer):
        inp = tokenizer(input, return_tensors="pt")
        X = inp["input_ids"].to(device)
        a = inp["attention_mask"].to(device)
        output = model.generate(X, attention_mask=a , max_length=300)
        output = tokenizer.decode(output[0])
        return remove_pad_token(output)

    pred_output_path_test = os.path.join(output_dir, "pred_test_data.txt")
    gold_output_path_test = os.path.join(output_dir, "gold_test_data.txt")
    pred_output_path_validation = os.path.join(output_dir, "pred_validation_data.txt")
    gold_output_path_validation = os.path.join(output_dir, "gold_validation_data.txt")

    def model_output(data, pred_output_path, gold_output_path):
        predicted_queries = []
        gold_queries = []
        for i in range(len(data)):
            example = data[i]
            db_id = example['db_id']
            query = example['query']
            schema = id_2_schema_dict[db_id]
            question = example['question']
            input = format_input(question, db_id, schema)
            print("Input: ", input)
            print("T Query: ",query)
            gold_queries.append(f"{query}\t{db_id}")
            predicted_result = infer(input, fine_tuned_model, fine_tuned_model_tokenizer)
            print("P Query: ", predicted_result)
            print("")
            predicted_queries.append(predicted_result)

        print("Writing pred quereis out")
        with open(pred_output_path, 'w') as fp:
            for item in predicted_queries:
                # write each item on a new line
                fp.write("%s\n" % item.encode('utf-8'))

        print("Writing gold quereis out")
        with open(gold_output_path, 'w') as fp:
            for item in gold_queries:
                # write each item on a new line
                fp.write("%s\n" % item.encode('utf-8'))

        return

    print("Inferring trained model for test set...")
    model_output(dataset_test, pred_output_path_test, gold_output_path_test)

    print("Inferring trained model for validation set...")
    model_output(dataset_validation, pred_output_path_validation, gold_output_path_validation)

    print("Finished inferring trained model for validation set...")
    print("ALL DONE!")

if __name__ == "__main__":
    main()

